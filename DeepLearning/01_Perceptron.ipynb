{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron: The Building Block of Neural Networks\n",
    "👨‍💻 **Author:** Zohaib Ahmad\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/GitHub-zohaibkk-blue?style=flat&logo=github)](https://github.com/zohaibkk)\n",
    "[![Kaggle](https://img.shields.io/badge/Kaggle-zohaibkk-blue?style=flat&logo=kaggle)](https://www.kaggle.com/zohaibkk)\n",
    "[![LinkedIn](https://img.shields.io/badge/LinkedIn-zohaibkk-blue?style=flat&logo=linkedin)](https://www.linkedin.com/in/zohaibkk/)\n",
    "## Introduction\n",
    "\n",
    "The Perceptron is one of the simplest artificial neural network architectures. It serves as the foundational unit in many neural network models. Developed by Frank Rosenblatt in 1957, the Perceptron mimics the functionality of a biological neuron, receiving input signals, processing them, and producing an output signal.\n",
    "\n",
    "## Structure\n",
    "\n",
    "The Perceptron consists of:\n",
    "1. **Input Layer:** Where the input signals are fed into the Perceptron.\n",
    "2. **Weights:** Each input signal is associated with a weight, which determines its importance.\n",
    "3. **Summation Function:** It computes the weighted sum of the input signals.\n",
    "4. **Activation Function:** It processes the weighted sum and produces the output signal.\n",
    "\n",
    "## Working Principle\n",
    "\n",
    "1. **Input Signals:** The input signals are numeric values representing various features of the input data.\n",
    "2. **Weights:** Each input signal is multiplied by a corresponding weight. These weights determine the significance of each input.\n",
    "3. **Summation:** The weighted sum of all input signals is calculated.\n",
    "   Weighted Sum = Σ_{i=1}^{n} (x_{i} \\cdot w_{i}) + b\n",
    "\n",
    "   where \\(x_i\\) is the \\(i^{th}\\) input, \\(w_i\\) is the corresponding weight, \\(n\\) is the number of inputs, and \\(b\\) is the bias term.\n",
    "4. **Activation:** The weighted sum is then passed through an activation function to produce the output signal. Common activation functions include the step function, sigmoid function, or rectified linear unit (ReLU).\n",
    "\n",
    "## Learning\n",
    "\n",
    "The Perceptron learns by adjusting its weights based on the error in its predictions. This process is known as the Perceptron Learning Rule or the Delta Rule. The goal is to minimize the difference between the predicted output and the actual output by updating the weights accordingly.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "While the Perceptron is a fundamental building block of neural networks, it has limitations. It can only learn linearly separable patterns and cannot solve problems that are not linearly separable. However, these limitations led to the development of more complex neural network architectures capable of handling nonlinear relationships.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Define the Perceptron class\n",
    "\n",
    "First, we need to create a class for the Perceptron. In Python, a class is like a blueprint for creating objects. Each object created from a class can have its own unique attributes and methods.\n",
    "\n",
    "Step 2: Initialize the Perceptron\n",
    "\n",
    "We'll define an initialization method (__init__) where we set up the initial state of the Perceptron. This includes initializing the weights and bias. We'll also allow for an optional parameter learning_rate to control how much the weights are adjusted during training.\n",
    "\n",
    "Step 3: Make Predictions\n",
    "\n",
    "We'll create a method predict to make predictions based on the input data. This method will calculate the weighted sum of the inputs, add the bias, and pass the result through an activation function to produce the output.\n",
    "\n",
    "Step 4: Define the Activation Function\n",
    "\n",
    "We need to define an activation function. Here, we'll use a step function, which returns 1 if the input is greater than or equal to 0, and 0 otherwise.\n",
    "\n",
    "Step 5: Train the Perceptron\n",
    "\n",
    "We'll implement a method train to train the Perceptron. This method will adjust the weights based on the error between the predicted output and the actual output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, num_inputs, learning_rate=0.01):\n",
    "        self.weights = np.random.rand(num_inputs)  # Initialize weights randomly\n",
    "        self.bias = np.random.rand()  # Initialize bias randomly\n",
    "        self.learning_rate = learning_rate  # Set the learning rate\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        weighted_sum = np.dot(inputs, self.weights) + self.bias  # Calculate the weighted sum\n",
    "        return self.activation_function(weighted_sum)  # Pass through activation function\n",
    "\n",
    "    def activation_function(self, x):\n",
    "        return 1 if x >= 0 else 0  # Step function as the activation function\n",
    "\n",
    "    def train(self, training_inputs, labels, epochs):\n",
    "        for epoch in range(epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)  # Make prediction\n",
    "                error = label - prediction  # Calculate error\n",
    "                self.weights += self.learning_rate * error * inputs  # Update weights\n",
    "                self.bias += self.learning_rate * error  # Update bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Using the Perceptron\n",
    "\n",
    "Now that we've defined our Perceptron class, we can create an instance of it and use it for making predictions and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define training data\n",
    "training_inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])  # Input features\n",
    "labels = np.array([0, 0, 0, 1])  # Corresponding labels\n",
    "\n",
    "# Create Perceptron instance\n",
    "num_inputs = 2  # Number of input features\n",
    "learning_rate = 0.01\n",
    "perceptron = Perceptron(num_inputs, learning_rate)\n",
    "\n",
    "# Train the Perceptron\n",
    "epochs = 100\n",
    "perceptron.train(training_inputs, labels, epochs)\n",
    "\n",
    "# Make predictions\n",
    "new_input = np.array([1, 0])  # New input data\n",
    "prediction = perceptron.predict(new_input)\n",
    "print(\"Prediction:\", prediction)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
